from model.fact_checker.fact_checker import *
from pipeline import *


class LLMMultiShotFactChecker(FactChecker, PipelineLLM, PipelineDemonstration):
    """
    A fact checker class that uses a Large Language Model (LLM) to compare
    sets of "answer triplets" against "reference triplets" and determine
    whether each answer triplet is True or False.
    """

    def __init__(self, config: dict, logger: logging.Logger):
        # Initialize all parent classes with the provided configuration
        FactChecker.__init__(self, config, logger)
        PipelineLLM.__init__(self, config)
        PipelineDemonstration.__init__(self, config)

    @property
    def directions(self):
        """
        Instructions provided to the LLM to guide the output format and logic.
        The output should map each answer triplet index to a boolean result (True or False).
        """
        return [
            "Answer only for the output",
            "output should be triplet_idx1:result1, triplet_idx2:result2, ...",
            "the output should be created along the input triplets",
            "the result should be one of True or False",
        ]

    def forward(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        return_prompt=False,
    ):
        """
        Perform a forward pass to fact-check the given answer triplets against reference triplets.

        Args:
            answer_triplets (list): The triplets generated by a model or user.
            reference_triplets (list): The ground-truth or reference triplets.

        Returns:
            tuple: A dictionary mapping each answer triplet index to a boolean (True/False),
                   and None as the second return value (for future extensions).
        """
        if self.config.model.fact_checker.split_reference_triplets:
            output_list = []
            for segment in reference_triplets:
                self.logger.debug(
                    "Segment: %s",
                    "\n-".join(
                        [
                            f"{idx} : {str(triplet)}"
                            for idx, triplet in enumerate(segment)
                        ]
                    ),
                )
                self.logger.debug("segment length: %s", len(segment))
                fact_check_prediction, _ = self.model_forward(
                    answer_triplets, segment, False  # temporal hard code
                )
                output_list.append(fact_check_prediction)
            return self.merge_segment_outputs(output_list), None
        else:
            reference_triplets = self.flatten_triplets(reference_triplets)
            return self.model_forward(
                answer_triplets, reference_triplets, return_prompt
            )

    def model_forward(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        return_prompt: bool = False,
    ):

        # Build the prompt for the model by formatting the input triplets
        if self.config.model.fact_checker.inquiry_mode:
            triplet_comparison_prompt = self.get_inquiry_model_prompt(
                answer_triplets=answer_triplets, reference_triplets=reference_triplets
            )
        else:
            triplet_comparison_prompt = self.get_model_prompt(
                answer_triplets=answer_triplets, reference_triplets=reference_triplets
            )
        # Invoke the LLM with the constructed prompt to get the raw matching result as text
        match_result = self.model.invoke(triplet_comparison_prompt).content
        # Parse the raw string output into a structured dictionary of triplet_idx: boolean_result
        if return_prompt:
            if self.config.model.fact_checker.inquiry_mode:
                return (
                    self.parse_triplet_comparison_inquiry_output(match_result),
                    None,
                    triplet_comparison_prompt,
                )
            else:
                return (
                    self.parse_triplet_comparison_output(match_result),
                    None,
                    triplet_comparison_prompt,
                )
        else:
            if self.config.model.fact_checker.inquiry_mode:
                return self.parse_triplet_comparison_inquiry_output(match_result), None
            else:
                return self.parse_triplet_comparison_output(match_result), None

    def get_model_prompt(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        **kwargs,
    ):

        examples = self.get_demo_data_by_idx(
            idx=9999,  # need to change here
            num_samples=self.config.model.fact_checker.num_shot,
            demo_type="fact_checker",
        )
        # Use the template message with the formatted input (answer and reference triplets)
        return self.message_list_template["n_shot_triplet_match_test_1"].invoke(
            input=self.triplet_comparison_input_formatter(
                answer_triplets=answer_triplets,
                reference_triplets=reference_triplets,
                examples=examples,
            )
        )

    def get_inquiry_model_prompt(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        **kwargs,
    ):

        examples = self.get_demo_data_by_idx(
            idx=9999,  # need to change here
            num_samples=self.config.model.fact_checker.num_shot,
            demo_type="fact_checker",
        )
        # Use the template message with the formatted input (answer and reference triplets)
        return self.message_list_template["n_shot_triplet_match_test_inquiry_3"].invoke(
            input=self.triplet_comparison_input_formatter(
                answer_triplets=answer_triplets,
                reference_triplets=reference_triplets,
                examples=examples,
            )
        )

    def triplet_comparison_input_formatter(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        examples: str,
    ):
        """
        Format answer and reference triplets into strings suitable for LLM input.

        Args:
            answer_triplets (list): The answer triplets to be checked.
            reference_triplets (list): The reference triplets to compare against.

        Returns:
            dict: A dictionary used to format the prompt template,
                  containing directions and two lists of triplets in text form.
        """
        return {
            "directions": "\n-".join(self.directions),
            "answer_triplets": "\n-".join(
                [
                    f"{idx}: " + str(input_triplet)
                    for idx, input_triplet in enumerate(answer_triplets)
                ]
            ),
            "reference_triplets": "\n-".join(
                [
                    f"{idx}: " + str(source_triplet)
                    for idx, source_triplet in enumerate(reference_triplets)
                ]
            ),
            "examples": examples,
        }

    def parse_triplet_comparison_output(self, string_output: str) -> dict:
        """
        Parse the raw string output from the LLM into a structured dictionary of triplet results.

        The output should match the format: triplet_idx:result (e.g., "0:True, 1:False").

        Args:
            string_output (str): The raw output string from the LLM.

        Returns:
            dict: A dictionary where keys are triplet indices (int) and
                  values are booleans indicating True/False for each triplet.
        """
        # Split the output by commas to separate each triplet's result
        splitted_string_outputs = string_output.replace("\n", ",").split(",")
        match_output = {}
        # Try to evaluate each part as a dictionary entry like "{0:True}"
        for splitted_string_output in splitted_string_outputs:
            try:
                # Remove potential hyphens and wrap in braces to form a valid Python dictionary entry
                match_output.update(
                    eval("{" + splitted_string_output.replace("-", "") + "}")
                )
            except Exception as e:
                # If parsing fails, skip that entry
                self.logger.warning(
                    f"Failed to parse fact checking output: '{string_output}'. Skipping it"
                )
                self.logger.debug("Error occured in : %s", string_output)
                pass
        return match_output

    def parse_triplet_comparison_inquiry_output(self, string_output: str) -> dict:
        """
        Parse the raw string output from the LLM into a structured dictionary of triplet results.

        The output should match the format: triplet_idx:result (e.g., "0:True, 1:False").

        Args:
            string_output (str): The raw output string from the LLM.

        Returns:
            dict: A dictionary where keys are triplet indices (int) and
                  values are booleans indicating True/False for each triplet.
        """
        # Split the output by commas to separate each triplet's result
        reference_triplets_part = string_output.split("[FINAL ANSWER]")[0]
        fact_check_result_part = string_output.split("[FINAL ANSWER]")[-1]
        splitted_string_outputs = (
            fact_check_result_part.replace("\n", ",")
            .replace("triplet_idx_", "")
            .replace("triplet_", "")
            .split(",")
        )
        match_output = {}
        # Try to evaluate each part as a dictionary entry like "{0:True}"
        if len(reference_triplets_part) > 0:
            self.logger.debug("Reference triplets: %s", reference_triplets_part)
        for splitted_string_output in splitted_string_outputs:
            try:
                # Remove potential hyphens and wrap in braces to form a valid Python dictionary entry
                match_output.update(
                    eval("{" + splitted_string_output.replace("-", "") + "}")
                )
            except Exception as e:
                # If parsing fails, skip that entry
                self.logger.warning(
                    f"Failed to parse fact checking output: '{fact_check_result_part}'. Skipping it"
                )
                self.logger.debug("Error occured in : %s", fact_check_result_part)
                pass
        return match_output
