from model.fact_checker.fact_checker import *
from pipeline import *


class LLMFactChecker(FactChecker, PipelineLLM, PipelinePrompt):
    """
    A fact checker class that uses a Large Language Model (LLM) to compare
    sets of "answer triplets" against "reference triplets" and determine
    whether each answer triplet is True or False.
    """

    def __init__(self, config: dict, logger: logging.Logger):
        # Initialize all parent classes with the provided configuration
        FactChecker.__init__(self, config, logger)
        PipelineLLM.__init__(self, config)
        PipelinePrompt.__init__(self, config)

    @property
    def directions(self):
        """
        Instructions provided to the LLM to guide the output format and logic.
        The output should map each answer triplet index to a boolean result (True or False).
        """
        return [
            "Answer only for the output",
            "output should be triplet_idx1:result1, triplet_idx2:result2, ...",
            "the output should be created along the input triplets",
            "the result should be one of True or False",
        ]

    def forward(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        return_prompt: bool = False,
    ):
        """
        Perform a forward pass to fact-check the given answer triplets against reference triplets.

        Args:
            answer_triplets (list): The triplets generated by a model or user.
            reference_triplets (list): The ground-truth or reference triplets.

        Returns:
            tuple: A dictionary mapping each answer triplet index to a boolean (True/False),
                   and None as the second return value (for future extensions).
        """
        if self.config.model.fact_checker.split_reference_triplets:
            output_list = []
            for segment in reference_triplets:
                fact_check_prediction, _ = self.model_forward(
                    answer_triplets, segment, False  # temporal hard code
                )
                output_list.append(fact_check_prediction)
            return self.merge_segment_outputs(output_list), None
        else:
            reference_triplets = self.flatten_triplets(reference_triplets)
            return self.model_forward(
                answer_triplets, reference_triplets, return_prompt
            )

    def model_forward(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        return_prompt: bool = False,
    ):

        # Build the prompt for the model by formatting the input triplets
        triplet_comparison_prompt = self.get_model_prompt(
            answer_triplets=answer_triplets, reference_triplets=reference_triplets
        )
        # Invoke the LLM with the constructed prompt to get the raw matching result as text
        match_result = self.model.invoke(triplet_comparison_prompt).content
        # Parse the raw string output into a structured dictionary of triplet_idx: boolean_result
        if return_prompt:
            return (
                self.parse_triplet_comparison_output(match_result),
                None,
                triplet_comparison_prompt,
            )
        else:
            return self.parse_triplet_comparison_output(match_result), None

    def get_model_prompt(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
        **kwargs,
    ):
        """
        Generate the model prompt by using a template and providing formatted input.

        Args:
            answer_triplets (list): The answer triplets to be checked.
            reference_triplets (list): The reference triplets for comparison.

        Returns:
            MessageList: A formatted prompt ready to be passed to the LLM.
        """
        # Use the template message with the formatted input (answer and reference triplets)
        return self.message_list_template["triplet_match_test"].invoke(
            input=self.triplet_comparison_input_formatter(
                answer_triplets, reference_triplets
            )
        )

    def triplet_comparison_input_formatter(
        self,
        answer_triplets: List[List],
        reference_triplets: List[List],
    ):
        """
        Format answer and reference triplets into strings suitable for LLM input.

        Args:
            answer_triplets (list): The answer triplets to be checked.
            reference_triplets (list): The reference triplets to compare against.

        Returns:
            dict: A dictionary used to format the prompt template,
                  containing directions and two lists of triplets in text form.
        """
        return {
            "directions": "\n-".join(self.directions),
            "answer_triplets": "\n-".join(
                [
                    f"{idx}: " + str(input_triplet)
                    for idx, input_triplet in enumerate(answer_triplets)
                ]
            ),
            "reference_triplets": "\n-".join(
                [str(source_triplet) for source_triplet in reference_triplets]
            ),
        }

    def parse_triplet_comparison_output(self, string_output: str) -> dict:
        """
        Parse the raw string output from the LLM into a structured dictionary of triplet results.

        The output should match the format: triplet_idx:result (e.g., "0:True, 1:False").

        Args:
            string_output (str): The raw output string from the LLM.

        Returns:
            dict: A dictionary where keys are triplet indices (int) and
                  values are booleans indicating True/False for each triplet.
        """
        # Split the output by commas to separate each triplet's result
        splitted_string_outputs = string_output.replace("\n", ",").split(",")

        match_output = {}
        # Try to evaluate each part as a dictionary entry like "{0:True}"
        for splitted_string_output in splitted_string_outputs:
            try:
                # Remove potential hyphens and wrap in braces to form a valid Python dictionary entry
                match_output.update(
                    eval("{" + splitted_string_output.replace("-", "") + "}")
                )
            except Exception as e:
                # If parsing fails, skip that entry
                self.logger.warning(
                    "Failed to parse the fact checker output : %s / %s",
                    str(e),
                    splitted_string_output,
                )
                self.logger.debug("Error occured in : %s", string_output)
                pass
        return match_output
