from utils.utils import *
from pipeline import *
from model import *
from typing import Dict, Any, List


class LLMFactCheckingSystem(PipelineBase):
    """
    A class representing a Retrieval-Augmented Generation (RAG) pipeline.
    This class does:
      1)integrates Five main components:an answer generator, a triplet generator, a fact checker, hallucination data generator, and a reprompter.
      2)This class defines the main process of our RAG pipeline.
      3)Defines methods needed in above process.
    """

    def __init__(self, config: dict, logger: logging.Logger):
        super().__init__(config)
        self.logger = logger
        self.answer_generator = model_name_class_mapping["answer_generator"][
            config.model.answer_generator.model_name
        ](config, logger)
        self.triplet_generator = model_name_class_mapping["triplet_generator"][
            config.model.triplet_generator.model_name
        ](config, logger)
        self.fact_checker = model_name_class_mapping["fact_checker"][
            config.model.fact_checker.model_name
        ](config, logger)
        self.reprompter = model_name_class_mapping["reprompter"][
            config.model.reprompter.model_name
        ](config, logger)
        self.hallucination_data_generator = model_name_class_mapping[
            "hallucination_data_generator"
        ][config.model.hallucination_data_generator.model_name](config, logger)

    def forward(self, data: Dict[str, Any]):
        """
        The whole process of actual forward pass of the system. Processes the input data and generates a response using the model.

        Args:
            data (Dict[str, Any]): A dictionary containing the following keys:
                - "reference_documents" (List[str]): A list of reference documents.
                - "question" (str): The question to be answered.
                - "reference_triplets" (Any): Additional data required for the model.

        Returns:
            Any: The output generated by the model based on the input data.
        """

        question_prompt = self.answer_generator.get_model_prompt(
            reference_documents=data["reference_documents"], question=data["question"]
        )

        return self.model_forward(question_prompt, data["reference_triplets"])

    def reprompter_forward(self, data: dict, output: dict):
        """
        Processes the forward pass for the reprompter model.

        Args:
            data (dict): A dictionary containing the input data, including:
                - question (str): The input question.
                - reference_documents (list): A list of reference documents.
                - reference_triplets (list): A list of reference triplets.
            output (dict): A dictionary containing the initial model output, including:
                - generated_answer (str): The generated answer from the model.
                - answer_triplets (list): A list of answer triplets.
                - fact_check_prediction_binary (bool): The binary prediction result of fact-checking.
            dataset: The dataset being used (not utilized in the current implementation).

        Returns:
            dict: A dictionary with keys prefixed by "reprompt_" containing the output from the reprompter model.
        """
        reprompt_prompt = self.reprompter.get_model_prompt(
            question=data["question"],
            generated_answer=output["generated_answer"],
            reference_documents=data["reference_documents"],
            answer_triplets=output["answer_triplets"],
            prediction_binary=output["fact_check_prediction_binary"],
        )

        output = self.model_forward(reprompt_prompt, data["reference_triplets"])

        return {f"reprompt_{k}": v for k, v in output.items()}

    def model_forward(self, question_prompt: str, source_triplets: List[List[str]]):
        """
        Forward pass of just models. Processes a question prompt and source triplets to generate an answer, extract triplets from the answer,
        and perform fact-checking.

        Args:
            question_prompt (str): The input question prompt to generate an answer for.
            source_triplets (List[List[str]]): A list of source triplets to fact-check the generated answer against.

        Returns:
            dict: A dictionary containing the following keys:
                - "fact_check_prediction_binary" (bool): The binary result of the fact-checking process.
                - "prediction_raw" (Any): The raw prediction result from the fact-checker.
                - "answer_triplets" (List[List[str]]): The triplets extracted from the generated answer.
                - "generated_answer" (str): The generated answer based on the question prompt.
                - "question_prompt" (str): The original question prompt.
        """
        generated_answer = self.answer_generator.forward(question_prompt)

        answer_triplets, triplet_generator_prompt = self.triplet_generator.forward(
            generated_answer, return_prompt=True
        )
        fact_check_prediction_binary, prediction_raw = self.fact_checker.forward(
            answer_triplets, source_triplets, return_prompt=False
        )
        return {
            "fact_check_prediction_binary": fact_check_prediction_binary,
            "answer_triplets": answer_triplets,
            "generated_answer": generated_answer,
        }

    def hlcntn_forward(self, data, hlcntn_data):
        """
        Perform forward pass for hallucination data fact-checking.

        Args:
            data (dict): A dictionary containing reference triplets.
                - "reference_triplets" (list): List of reference triplets.
            hlcntn_data (dict): A dictionary containing hallucination data.
                - "answer_triplets" (list): List of answer triplets generated from hallucination data.
                - "generated_answer" (str): The generated answer from hallucination data.

        Returns:
            dict: A dictionary containing the fact-checking results.
                - "fact_check_prediction_binary" (bool): The binary prediction result of the fact-checker.
                - "prediction_raw" (any): The raw prediction result of the fact-checker.
                - "answer_triplets" (list): The answer triplets from hallucination data.
                - "generated_answer" (str): The generated answer from hallucination data.
        """
        prediction_binary, prediction_raw = self.fact_checker.forward(
            hlcntn_data[
                "answer_triplets"
            ],  # we should change the first input if we want to generate triplet from hallucination data at inference time
            data["reference_triplets"],
            return_prompt=False,
        )
        return {
            "fact_check_prediction_binary": prediction_binary,
            "answer_triplets": hlcntn_data["answer_triplets"],
            "generated_answer": hlcntn_data["generated_answer"],
        }

    def direct_text_match_forward(self, answer_text, reference_text):
        """
        input:
            - answer_text: The text to be checked.
            - reference_text: The reference text to compare against.
        output:
            -  the triplets from the answer text
            -  the fact checker output
            -  which triplet is predicted as False
        """

        answer_triplets = self.triplet_generator.forward(
            answer_text, return_prompt=False
        )

        reference_triplets = self.triplet_generator.forward(
            reference_text, return_prompt=False
        )

        fact_check_prediction_binary, prediction_raw = self.fact_checker.forward(
            answer_triplets, reference_triplets, return_prompt=False
        )

        return {
            "answer_triplets": answer_triplets,
            "reference_triplets": reference_triplets,
            "fact_check_prediction_binary": fact_check_prediction_binary,
        }
